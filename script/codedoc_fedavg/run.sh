python -m main.codedoc_fedavg \
  --client_num_in_total 15 \
  --client_num_per_round 5 \
  --comm_round 10 \
  --dataset "codedoc" \
  --language "python" \
  --train_data_file "data/store/codedoc/python/train.jsonl" \
  --train_partition_file "data/store/codedoc/python/train_partition.pk" \
  --eval_data_file "data/store/codedoc/python/eval.jsonl" \
  --partition_method niid_quantity_clients=15_beta=1.0 \
  --fl_algorithm FedAvg \
  --model_type "roberta-base" \
  --model_name "microsoft/codebert-base" \
  --do_lower_case True \
  --train_batch_size 32 \
  --eval_batch_size 32 \
  --max_seq_length 256 \
  --max_target_length 128 \
  --learning_rate 5e-5 \
  --beam_size 10 \
  --epochs 3 \
  --output_dir "tmp/fedavg_codedoc_output/" \
  --cache_dir "cache/codedoc/python" \
  --do_train \
  --do_eval

python -m main.codedoc_fedavg \
  --client_num_in_total 15 \
  --client_num_per_round 5 \
  --comm_round 10 \
  --dataset "codedoc" \
  --language "java" \
  --train_data_file "data/store/codedoc/java/train.jsonl" \
  --train_partition_file "data/store/codedoc/java/train_partition.pk" \
  --eval_data_file "data/store/codedoc/java/valid.jsonl" \
  --partition_method niid_quantity_clients=15_beta=1.0 \
  --fl_algorithm FedAvg \
  --model_type "roberta-base" \
  --model_name "microsoft/codebert-base" \
  --do_lower_case True \
  --train_batch_size 32 \
  --eval_batch_size 32 \
  --max_seq_length 256 \
  --max_target_length 128 \
  --learning_rate 5e-5 \
  --beam_size 10 \
  --epochs 3 \
  --output_dir "tmp/fedavg_codedoc_output/" \
  --cache_dir "cache/codedoc/java" \
  --do_train \
  --do_eval